{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e507be46",
   "metadata": {},
   "source": [
    "# (Purpose) Build a machine learning SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43248ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_DATA = False    \n",
    "# SCALE_DATA = True \n",
    "\n",
    "# ---------------------- TCGA dataset ---------------------------              \n",
    "PATH_TO_TCGA_DATA  = \"data/preprocessing_combinations/tcga_unscaled_unnormalized_nobatchcorrection__mockData.tsv\"                 \n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_unnormalized_nobatchcorrection.tsv\"  # realData0\n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_unnormalized_batchcorrected.tsv\"     # realData1 \n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_unnormalized_batchcorrected_protocol.tsv\"      \n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_unnormalized_batchcorrected_disease.tsv\"     \n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_unnormalized_batchcorrected_consortium.tsv\"   \n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_unnormalized_batchcorrected_protocoldisease.tsv\"   \n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_unnormalized_batchcorrected_protocolconsortium.tsv\"   \n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_unnormalized_batchcorrected_diseaseconsortium.tsv\"   \n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_qn_nobatchcorrection.tsv\"            # realData2\n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_qn_batchcorrected.tsv\"               # realData3\n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_qntarget_nobatchcorrection.tsv\"      # realData4\n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_qntarget_batchcorrected.tsv\"         # realData5\n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_fsqn_nobatchcorrection.tsv\"          # realData6\n",
    "# PATH_TO_TCGA_DATA = \"data/preprocessing_combinations/tcga_unscaled_fsqn_batchcorrected.tsv\"             # realData7    \n",
    "    \n",
    "    \n",
    "# ---------------------- GTEx dataset ---------------------------     \n",
    "PATH_TO_GTEX_DATA  = \"data/preprocessing_combinations/gtex_unscaled_unnormalized_nobatchcorrection__mockData.tsv\"                 \n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_unnormalized_nobatchcorrection.tsv\"  # realData0             \n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_unnormalized_batchcorrected.tsv\"     # realData1            \n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_unnormalized_batchcorrected_protocol.tsv\"                \n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_unnormalized_batchcorrected_disease.tsv\"              \n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_unnormalized_batchcorrected_consortium.tsv\" \n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_unnormalized_batchcorrected_protocoldisease.tsv\"   \n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_unnormalized_batchcorrected_protocolconsortium.tsv\"   \n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_unnormalized_batchcorrected_diseaseconsortium.tsv\"                    \n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_qn_nobatchcorrection.tsv\"            # realData2\n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_qn_batchcorrected.tsv\"               # realData3\n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_qntarget_nobatchcorrection.tsv\"      # realData4 \n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_qntarget_batchcorrected.tsv\"         # realData5             \n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_fsqn_nobatchcorrection.tsv\"          # realData6\n",
    "# PATH_TO_GTEX_DATA = \"data/preprocessing_combinations/gtex_unscaled_fsqn_batchcorrected.tsv\"             # realData7                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b49031",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_FOR_RANDOM_STATE = 12345\n",
    "MAX_JOBS = 20          # use -3; this uses all possible except for 2 cores; if use (-1) then all cores used\n",
    "\n",
    "EXPLAINED_VARIANCE_PERCENT = 80\n",
    "\n",
    "# SCALER_TYPE = \"StandardScaler\"\n",
    "SCALER_TYPE = \"MinMaxScaler\"\n",
    "\n",
    "PATH_TO_UMAP_PLOTS   = \"results/classification_UMAP_PLOTS/\" + current_date \n",
    "PATH_TO_UMAP_PLOTS_WITHOUT_A_TITLE   = \"results/classification_UMAP_PLOTS/pdfs_without_a_title/\" + current_date \n",
    "PATH_TO_PCA_PLOTS   = \"results/classification_PCA_PLOTS/\" + current_date \n",
    "PATH_TO_PCA_PLOTS_WITHOUT_A_TITLE   = \"results/classification_PCA_PLOTS/pdfs_without_a_title/\" + current_date \n",
    "PATH_TO_CV_RESULTS  = \"results/classification_CV_RESULTS/\" + current_date \n",
    "PATH_TO_PREDICTIONS = \"results/classification_PREDICTIONS/\" + current_date \n",
    "PATH_TO_PREDICTIONS_EXCEL_FILES = \"results/classification_PREDICTIONS/excel_files/\" + current_date "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f0f9c",
   "metadata": {},
   "source": [
    "## Import packages required in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c151c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  begin -- importing Python packages\")\n",
    "\n",
    "import numpy as np              \n",
    "import pandas as pd             \n",
    "import matplotlib               \n",
    "import matplotlib.cm            \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import joblib                   \n",
    "        \n",
    "import os                       \n",
    "import umap.umap_ as umap       \n",
    "import openpyxl                 \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder            \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef                \n",
    "                                \n",
    "\n",
    "                \n",
    "    \n",
    "print(\"  done  -- importing Python packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a343ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  Checking the number of cores in the current system (will be using all)\")    \n",
    "n_cpu = os.cpu_count()\n",
    "print(\"    number of CPUs in the system:\", n_cpu)\n",
    "print(\"    number of CPUS will use during grid search:\", MAX_JOBS)\n",
    "\n",
    "# print(\"  Enabling 'forkserver' option for multithreading to prevent freezes with too many cores\")\n",
    "# # (source) https://scikit-learn.org/stable/faq.html#why-do-i-sometime-get-a-crash-freeze-with-n-jobs-1-under-osx-or-linux\n",
    "# multiprocessing.set_start_method('forkserver')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db0cb5",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36aab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  Trying to load tcga data (** takes about 7 minutes **)\")  \n",
    "df = None\n",
    "\n",
    "print(\"[TIMESTAMP]\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "try:\n",
    "    df = pd.read_table(PATH_TO_TCGA_DATA)\n",
    "    \n",
    "    print (\"    File(s) found\")\n",
    "    print (\"      pd.read_table() into 'df':\", PATH_TO_TCGA_DATA)\n",
    "except:\n",
    "    print (\"    ERROR:  File(s) not found\")\n",
    "    #quit()\n",
    "    \n",
    "print(\"  checking dimensions of the file loaded into df\")  \n",
    "print(\"    df.shape:\", df.shape)\n",
    "print(\"[TIMESTAMP]\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# keep a backup copy of what was loaded \n",
    "df_backup = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR DEBUG ONLY - manually comment out to reset the loaded df object before any modifications\n",
    "# df = df_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8807c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  tallying the 'label' column of the loaded dataframe\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  tallying the 'label' column of the loaded dataframe(possible after subsetting out less samples)\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc988bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  previewing the top of the file\")\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a679894",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  previewing the bottom of the file\")\n",
    "print(df.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daab9cb",
   "metadata": {},
   "source": [
    "## Save first two columns (sample_id, label) and drop them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b1953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"  saving non-feature columns (sample_id, label) elsewhere, they need to be dropped from df for other functions\")\n",
    "\n",
    "sample_ids = df['sample_id']\n",
    "labels = df['label']\n",
    "\n",
    "df = df.drop('sample_id', axis=1)  # axis=0 refers to vertical/rows\n",
    "df = df.drop('label', axis=1)      # axis=1 refers to horizontal/columns\n",
    "\n",
    "print(\"  checking dimensions of the loaded df after dropping\")  \n",
    "print(\"    df.shape:\", df.shape, \"  # should be two columns shorter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4333d",
   "metadata": {},
   "source": [
    "## Perform a 80:20 split on the datasets, proportionally\n",
    "- (**NOTE**) this split, for all preprocessed combinations, is done outside of this notebook for datasets that are normalized or batch corrected\n",
    "- [not_used_anymore_but_for_python_ref] \n",
    "    * (source for split) https://stackoverflow.com/questions/64353575/train-test-split-preserving-class-proportions-in-each-split\n",
    "    * (source for subsetting df with list of indices) https://stackoverflow.com/questions/19155718/select-pandas-rows-based-on-list-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  [TCGA dataset already split] assigning the TCGA data into 80% train and 20% test sets\")  \n",
    "percent = 0.80\n",
    "\n",
    "X_train = df.head(int(len(df)*percent))\n",
    "Y_train = labels.head(int(len(df)*percent))\n",
    "id_train= sample_ids.head(int(len(df)*percent))\n",
    "X_test = df.iloc[max(X_train.index)+1:]\n",
    "Y_test = labels.iloc[max(Y_train.index)+1:]\n",
    "id_test= sample_ids.iloc[max(Y_train.index)+1:]\n",
    "    \n",
    "print(\"  checking dimensions of the new variables after train-test split\")  \n",
    "print(\"    X_train.shape:\", X_train.shape, \"  # should be 80% of data\")\n",
    "print(\"    Y_train.shape:\", Y_train.shape, \"      # should be 80% of labels\")\n",
    "print(\"    id_train.shape:\", id_train.shape, \"    # should be 80% of labels\")\n",
    "print(\"    X_test.shape :\", X_test.shape,  \"   # should be 20% of data\")\n",
    "print(\"    Y_test.shape :\", Y_test.shape,  \"       # should be 20% of labels\")    \n",
    "print(\"    id_test.shape :\", id_test.shape,  \"     # should be 20% of labels\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea59784",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  Creating the label encoder which is required for ML algorithms to know the ground truth\")\n",
    "label_encoder = LabelEncoder().fit(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  previewing 'X_train' that contains the training set data\")\n",
    "print(X_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88005da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"  tallying the different number of 'labels' among training set data\")\n",
    "print(Y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0093778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  previewing 'Y_train' that contains the training set labels\")\n",
    "print(Y_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  previewing 'X_test' that contains the test set data\")\n",
    "print(X_test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9a522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"  tallying the different number of 'labels' among test set data\")\n",
    "print(Y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  previewing 'Y_test' that contains the test set labels\")\n",
    "print(Y_test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fab58a",
   "metadata": {},
   "source": [
    "## Function declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0733f5",
   "metadata": {},
   "source": [
    "### (Helper functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ea408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (returns) if found then print the (rows=sample_id) and (cols=gene names) that are zero\n",
    "\n",
    "def check_if_any_genes_zero (arg_data, arg_sample_ids, arg_debug_filename):\n",
    "    print(\"  begin check_if_any_genes_zero()\")\n",
    "    \n",
    "    # remember that arg_data has first two cols as sample_id, label, then rest are genes; rows are each sample\n",
    "    \n",
    "    # work with a copy just in case and also assign row names as first column\n",
    "    tmp_df = arg_data.copy()\n",
    "    id_column = pd.DataFrame(data = arg_sample_ids)\n",
    "    tmp_df.index = id_column['sample_id']\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"[DEBUG]---------------------------------[DEBUG]\")\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "#     ## working - checks all elements and prints out the location with zero value\n",
    "#     for idx in arg_data.index:\n",
    "#         for column in arg_data.columns:\n",
    "#             if arg_data.loc[idx, column] == 0:\n",
    "#                 print(f'  + found zero @ ({idx},{column}): {arg_data.loc[idx, column]}')\n",
    "\n",
    "#     # (for testing only) set all value of this gene equal to zero\n",
    "#     arg_data['ENSG00000000419'] = 0\n",
    "    \n",
    "    \n",
    "    ## working - checks for columns(=genes) where all values zero and prints out the name of gene\n",
    "    for column in tmp_df.columns:\n",
    "        if (tmp_df[column] == 0).all():\n",
    "            # print(f'  + found all zeroes in col @ {column}')\n",
    "            counter = counter + 1\n",
    "        \n",
    "    print(f'  + [Total genes found with all zeros] {counter}')   \n",
    "    \n",
    "#     # be sure that you want to save the snapshots of files, because each one is many GBs and takes up lots of space!    \n",
    "#     print(f'  + writing {arg_debug_filename} to file under results/debug/')\n",
    "#     words = [PATH_TO_DEBUG, arg_debug_filename, \".tsv\"]\n",
    "#     save_filename_w_path_debug = \"\".join(words)\n",
    "#     tmp_df.to_csv(save_filename_w_path_debug, sep=\"\\t\")\n",
    "    \n",
    "    print(\"[END_DEBUG]-------------------------[END_DEBUG]\")\n",
    "    print(\"\")\n",
    "    \n",
    "    \n",
    "    print(\"  end check_if_any_genes_zero()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72670cd9",
   "metadata": {},
   "source": [
    "###     (Scaling function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be29928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (consider; this scaling might be done before other normalization steps? would need to be done outside this notebook)\n",
    "    \n",
    "# (returns) scaled version of data passed it; the scaling factors used   \n",
    "# (source) https://www.geeksforgeeks.org/python-how-and-where-to-apply-feature-scaling/\n",
    "# Scale the dataset; This is very important before you apply PCA\n",
    "# Also very important that the same scaling factor is used in both sets, otherwise get erroreous test set\n",
    "def scale_this_data(arg_data, arg_use_scalar, arg_scale_required): \n",
    "    \n",
    "    print(\"  begin scale_this_data()\")\n",
    "    print(\"   checking statistics of data before scaling\")  \n",
    "    print(\"    mean:\", np.mean(arg_data))\n",
    "    print(\"    max :\", np.amax(arg_data))\n",
    "    print(\"    min :\", np.amin(arg_data))\n",
    "#     print(\"    std :\", np.std(arg_data))\n",
    "\n",
    "    sc = None\n",
    "    \n",
    "    if arg_scale_required == True:\n",
    "        \n",
    "        print(\"   SCALE_DATA equals True, so we are going to scale data\")  \n",
    "        \n",
    "        if arg_use_scalar == None:\n",
    "            print(\"   No previous scaling factors, so will create new one and fit it to data\")\n",
    "            \n",
    "            if SCALER_TYPE == \"StandardScaler\":\n",
    "                print(\"   SCALER_TYPE is 'StandardScaler', now creating an instance of StandardScaler\")\n",
    "                sc = StandardScaler()\n",
    "            elif SCALER_TYPE == \"MinMaxScaler\":\n",
    "                print(\"   SCALER_TYPE is 'StandardScaler', now creating an instance of MinMaxScaler\")\n",
    "                sc = MinMaxScaler(feature_range=(0,1))\n",
    "            \n",
    "            sc.fit(arg_data)   \n",
    "        else:\n",
    "            print(\"   Previous scalar passed into function, so will use it\")\n",
    "            sc = arg_use_scalar\n",
    "\n",
    "        print(\"   scaling the data now\")\n",
    "        data_after_scaling = sc.transform(arg_data)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"   SCALE_DATA equals False, so we are NOT going to scale data\")  \n",
    "        data_after_scaling = arg_data\n",
    "        \n",
    "        if arg_use_scalar == None:\n",
    "            print(\"   No previous scaling factors, so will create new one but it wont be used\")\n",
    "            \n",
    "            if SCALER_TYPE == \"StandardScaler\":\n",
    "                print(\"   SCALER_TYPE is 'StandardScaler', now creating an instance of StandardScaler\")\n",
    "                sc = StandardScaler()\n",
    "            elif SCALER_TYPE == \"MinMaxScaler\":\n",
    "                print(\"   SCALER_TYPE is 'MinMaxScaler', now creating an instance of MinMaxScaler\")\n",
    "                sc = MinMaxScaler(feature_range=(0,1))\n",
    "            \n",
    "\n",
    "    print(\"  checking statistics of data after scaling\")  \n",
    "    print(\"    mean:\", np.mean(data_after_scaling))\n",
    "    print(\"    max :\", np.amax(data_after_scaling))\n",
    "    print(\"    min :\", np.amin(data_after_scaling))\n",
    "    # print(\"    std :\", np.std(data_after_scaling))\n",
    "\n",
    "    print(\"  end scale_this_data()\")\n",
    "    \n",
    "    return pd.DataFrame(data = data_after_scaling), sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3550cc1",
   "metadata": {},
   "source": [
    "###     (Dimension Reduction functions)\n",
    "\n",
    "(ex. **reducers**) PCA, UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f05bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (returns) modified DF and the reduce object (that was used for umap.transform())\n",
    "# (source on UMAP implementation) https://umap-learn.readthedocs.io/en/latest/basic_usage.html\n",
    "# (source on PCA implementation) https://www.datacamp.com/tutorial/principal-component-analysis-in-python\n",
    "# (source on explained variance) https://vitalflux.com/pca-explained-variance-concept-python-example/\n",
    "# (source on tick marks) https://stackoverflow.com/questions/30914462/how-to-force-integer-tick-labels\n",
    "\n",
    "def apply_feature_reduction(arg_data, arg_labels, arg_use_reducer, arg_reducer_type, arg_report_suffix):  \n",
    "    print(\"  begin apply_feature_reduction() with reducer_type:\", arg_reducer_type)\n",
    "    print(\"   checking dimensions of data before reducer\")       \n",
    "    print(\"    arg_data.shape:\", arg_data.shape)    \n",
    "    print(\"    arg_labels.shape:\", arg_labels.shape)                      \n",
    "    print(\"   checking statistics of data before reducer\")  \n",
    "    print(\"    mean:\", np.mean(arg_data))\n",
    "    print(\"    max :\", np.amax(arg_data))\n",
    "    print(\"    min :\", np.amin(arg_data))\n",
    "#     print(\"    std :\", np.std(arg_data))\n",
    "\n",
    "    reducer = None\n",
    "    exp_var_pca = None\n",
    "\n",
    "    if arg_use_reducer == None:\n",
    "        print(\"   No previous reducer object, so will create new one and fit it to data\")\n",
    "        \n",
    "        \n",
    "        if arg_reducer_type == \"umap\":\n",
    "            reducer = umap.UMAP(\n",
    "\n",
    "                random_state=SEED_FOR_RANDOM_STATE\n",
    "             )\n",
    "            reducer.fit(arg_data)       # this call updates the `pca` object  \n",
    "        elif arg_reducer_type == \"pca\":\n",
    "            reducer = PCA(\n",
    "                EXPLAINED_VARIANCE_PERCENT * 0.01,   # here we explicitly mention how much variance we would like PCA \n",
    "                                   #to capture and hence, the n_components will vary based on the variance parameter.\n",
    "                random_state=SEED_FOR_RANDOM_STATE\n",
    "             )\n",
    "            reducer.fit(arg_data)       # this call updates the `pca` object   \n",
    "\n",
    "            print(\"   applying pca.transform on new pca object\")\n",
    "            data_after_reducer = reducer.transform(arg_data)\n",
    "            \n",
    "            print(\"   now have the number of features to reduce to: \", str(data_after_reducer.shape[1]))\n",
    "            \n",
    "            print(\"   rebuilding the pca object with n_features (important for applying on test data later) \")\n",
    "            reducer = PCA(\n",
    "                n_components = data_after_reducer.shape[1],\n",
    "                random_state=SEED_FOR_RANDOM_STATE\n",
    "            )\n",
    "            reducer.fit(arg_data)       # this call updates the `pca` object   \n",
    "            \n",
    "            print(\"   applying pca.transform on new pca object (again, to be sure)\")\n",
    "            data_after_reducer = reducer.transform(arg_data)\n",
    "\n",
    "            print(\"   Generate the scree plot \")\n",
    "            # Determine explained variance using attribute of pca object\n",
    "            exp_var_pca = reducer.explained_variance_ratio_\n",
    "\n",
    "            # Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "            # for visualizing the variance explained by each principal component.\n",
    "            cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "            # Create the visualization plot for explained variaince \n",
    "            #   - barplot is used to represent individual explained variances \n",
    "            #   - step plot is used to represent the variance explained by different principal components\n",
    "            fig, ax = plt.subplots(1, figsize=(7, 5))\n",
    "            plt.bar(\n",
    "                    range(0,len(exp_var_pca)), \n",
    "                    exp_var_pca, \n",
    "                    alpha=0.5, \n",
    "                    align='center', \n",
    "                    label='Individual explained variance'\n",
    "            )\n",
    "            plt.step(\n",
    "                    range(0,len(cum_sum_eigenvalues)), \n",
    "                    cum_sum_eigenvalues, \n",
    "                    where='mid',\n",
    "                    label='Cumulative explained variance',\n",
    "                    color='darkgreen'\n",
    "                    )\n",
    "            plt.ylabel('Explained variance ratio')    \n",
    "            plt.ylim((0,1))\n",
    "            plt.yticks(np.arange(0, 1, 0.1))\n",
    "            plt.xlabel('Principal component index')\n",
    "            ax.xaxis.set_major_locator(MaxNLocator())\n",
    "            ax.yaxis.set_major_locator(MaxNLocator())\n",
    "            ax.margins(y=.1, x=.1)\n",
    "            plt.legend(loc='best')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            print(\"   Saving a figure for pca_plot_of_explained_variance\")\n",
    "            pca_plot_filename_wo_extension = \"_pca_plot_of_explained_variance\"\n",
    "            \n",
    "            # save set of figures as pdfs without titles as pdfs for manuscript\n",
    "            plt.savefig(PATH_TO_PCA_PLOTS_WITHOUT_A_TITLE + pca_plot_filename_wo_extension + \".pdf\", bbox_inches='tight')\n",
    "            \n",
    "            # save one set of figures as jpgs with titles that are useful in ppt slides\n",
    "            plt.title(\"{}% of the variance explained by {} principal components\".format(\n",
    "                                                            str(EXPLAINED_VARIANCE_PERCENT),\n",
    "                                                            data_after_reducer.shape[1],), \n",
    "                           fontsize=14\n",
    "                     )\n",
    "            plt.savefig(PATH_TO_PCA_PLOTS + pca_plot_filename_wo_extension + \".jpg\", bbox_inches='tight')\n",
    "\n",
    "            plt.close()\n",
    "        \n",
    "        else:\n",
    "            print(\"ERROR: Not implemented \")\n",
    "        \n",
    "        print(\"   applying reducer.transform on new reducer object\")\n",
    "        data_after_reducer = reducer.transform(arg_data)\n",
    "        \n",
    "       \n",
    "    else:\n",
    "        print(\"   Previous reducer object passed into function, so will use it\")\n",
    "        reducer = arg_use_reducer\n",
    "        print(\"   applying reducer.transform on passed in reducer object\")\n",
    "        data_after_reducer = reducer.transform(arg_data)\n",
    "        \n",
    "    \n",
    "\n",
    "    # Create the visualization plot \n",
    "    fig, ax = plt.subplots(1, figsize=(7, 5))\n",
    "    figure_labels = pd.Series(pd.Categorical(arg_labels, categories=reversed(np.unique(labels)), ordered=True))\n",
    "#     common_cmap = matplotlib.cm.get_cmap('tab20')  # will be deprecated\n",
    "    common_cmap = matplotlib.colormaps['tab20']\n",
    "    common_norm = plt.Normalize(vmin=0, vmax=len(np.unique(labels))-1)\n",
    "    scatter = plt.scatter(\n",
    "                data_after_reducer[:, 0],                 # x-axis data\n",
    "                data_after_reducer[:, 1],                 # y-axis data\n",
    "                s= 20,                                    # marker size\n",
    "                c=figure_labels.cat.codes.astype(float),     # list of colors\n",
    "                norm=common_norm,                         # normalization method used to scale scalar data to [0,1] \n",
    "                cmap=common_cmap,                         # Colormap instance used to map scalar data to colors\n",
    "                alpha=0.5\n",
    "               )\n",
    "\n",
    "    cbar = fig.colorbar(scatter, \n",
    "                    ax=ax, \n",
    "                    boundaries=np.arange(figure_labels.cat.categories.size+1)-0.5,\n",
    "                    orientation='vertical'\n",
    "                   )\n",
    "    cbar.set_ticks(np.arange(len(np.unique(labels))))\n",
    "    cbar.set_ticklabels(figure_labels.cat.categories)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.margins(y=.1, x=.1)\n",
    "\n",
    "    if arg_reducer_type == \"umap\":\n",
    "        print(\"   Saving a figure for umap:\", arg_report_suffix)\n",
    "        \n",
    "        plt.xlabel (\"UMAP1\")\n",
    "        plt.ylabel (\"UMAP2\")\n",
    "        \n",
    "        # save set of figures as pdfs without titles as pdfs for manuscript\n",
    "        plt.savefig(PATH_TO_UMAP_PLOTS_WITHOUT_A_TITLE + \"_umap_plot_of_\"+arg_report_suffix +\".pdf\")\n",
    "\n",
    "        # save one set of figures as jpgs with titles that are useful in ppt slides\n",
    "        plt.title(\"UMAP projection of \" + arg_report_suffix, fontsize=12);\n",
    "        plt.savefig(PATH_TO_UMAP_PLOTS + \"_umap_plot_of_\"+arg_report_suffix +\".jpg\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    elif arg_reducer_type == \"pca\":\n",
    "        exp_var_pca = reducer.explained_variance_ratio_\n",
    "        \n",
    "        print(\"   Saving a figure for pca_plot_of_PC1_vs_PC2:\", arg_report_suffix)\n",
    "        \n",
    "        plt.xlabel (\"PC1 (\" + str(int(exp_var_pca[0]*100)) + \"%)\")\n",
    "        plt.ylabel (\"PC2 (\" + str(int(exp_var_pca[1]*100)) + \"%)\")\n",
    "        \n",
    "        # save set of figures as pdfs without titles as pdfs for manuscript\n",
    "        plt.savefig(PATH_TO_PCA_PLOTS_WITHOUT_A_TITLE + \"_pca_plot_of_\"+arg_report_suffix +\"_PC1_vs_PC2.pdf\")\n",
    "\n",
    "        # save one set of figures as jpgs with titles that are useful in ppt slides        \n",
    "        plt.title(\"PC1 vs PC2 of \" + arg_report_suffix, fontsize=12);\n",
    "        plt.savefig(PATH_TO_PCA_PLOTS + \"_pca_plot_of_\"+arg_report_suffix +\"_PC1_vs_PC2.jpg\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    else:\n",
    "        print(\"ERROR: Not implemented \")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"   checking dimensions of data after reducer\")       \n",
    "    print(\"    data_after_reducerpca.shape: \", data_after_reducer.shape)    \n",
    "    print(\"    *notice* # of features reduced to: \", data_after_reducer.shape[1]) \n",
    "    print(\"    arg_labels.shape: \", arg_labels.shape)        \n",
    "    print(\"   checking statistics of data after reducer\")  \n",
    "    print(\"    mean: \", np.mean(data_after_reducer))\n",
    "    print(\"    max : \", np.amax(data_after_reducer))\n",
    "    print(\"    min : \", np.amin(data_after_reducer))\n",
    "#     print(\"    std :\", np.std(data_after_reducer))        \n",
    "    print(\"  end apply_feature_reduction() with of type \",arg_reducer_type )\n",
    "\n",
    "    # return the reducer-applied dataset and reducer object\n",
    "    return pd.DataFrame(data = data_after_reducer), reducer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbacd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates classifier models on training dataset (80% TCGA) \n",
    "# - updated one that calls to scaling and feature reduction\n",
    "\n",
    "# (source for passing in multiple variables via **best_params) \n",
    "# https://yasoob.me/2013/08/04/args-and-kwargs-in-python-explained/\n",
    "\n",
    "# how called\n",
    "\"\"\"\n",
    "    models = build_classifier_model(X_train_after_DR)\n",
    "\"\"\"\n",
    "\n",
    "def build_classifier_model(arg_train_data, arg_train_labels, arg_train_ids):    \n",
    "    encoded_y = pd.DataFrame(data={\"Type\":label_encoder.transform(arg_train_labels)})  #Y_train here was 'target' originally\n",
    "    best_gs_models = [None] * grids_n \n",
    "    \n",
    "    # need to keep track of these for later\n",
    "    current_best_model = []\n",
    "    current_best_scalar = []\n",
    "    current_best_umap = []\n",
    "    current_best_pca = []\n",
    "#     current_best_model = None     #changing from single best model to best models of each outer folder\n",
    "#     current_best_scalar = None\n",
    "#     current_best_umap = None\n",
    "#     current_best_pca = None\n",
    "\n",
    "    \n",
    "    # Construct pipelines for each algorithm\n",
    "    pipe_svm = Pipeline([('clf', SVC(probability = True, \n",
    "                                     random_state=SEED_FOR_RANDOM_STATE, \n",
    "                                     break_ties=True, \n",
    "                                     decision_function_shape='ovr',\n",
    "                                     class_weight='balanced',\n",
    "                                     cache_size=1000)\n",
    "                         )])\n",
    "#     pipe_svm = Pipeline([('clf', SVC(probability = True, \n",
    "#                                      random_state=SEED_FOR_RANDOM_STATE, \n",
    "#                                      break_ties=True, \n",
    "#                                      decision_function_shape='ovr', \n",
    "#                                      cache_size=1000, \n",
    "#                                      tol=1e-4)\n",
    "#                          )])\n",
    "\n",
    "\n",
    "    # Initialize the parameters required for the models\n",
    "    grid_params_svm = [{'clf__kernel': ['rbf', 'linear'],\n",
    "                        'clf__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "#                         'clf__gamma': [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "                       }]\n",
    "#     grid_params_svm = [{'clf__kernel': ['linear','rbf'],\n",
    "#                         'clf__C': [0.5,1,5,7.5,10,20,30,100],\n",
    "#                         'clf__gamma': [1e-5, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "#                         'clf__class_weight': [None, 'balanced'],\n",
    "#                        }]\n",
    "\n",
    "    # create the CV splitter\n",
    "    outer_k = 5\n",
    "    inner_k = 5\n",
    "    inner_cv_skf = StratifiedKFold(n_splits=inner_k, shuffle=True,  random_state=SEED_FOR_RANDOM_STATE)\n",
    "    outer_cv_skf = StratifiedKFold(n_splits=outer_k, shuffle=True,  random_state=SEED_FOR_RANDOM_STATE)\n",
    "   \n",
    "\n",
    "    # outer CV to estimate generalization error     \n",
    "    non_nested_scores = np.zeros((grids_n, outer_k))\n",
    "    nested_scores = np.zeros((grids_n, outer_k))\n",
    "    \n",
    "    # create new files to keep track of cross validation results on train data\n",
    "    with open(PATH_TO_CV_RESULTS+\"_SVM_build_classifier_results_of_inner_fold.txt\",'w') as f:\n",
    "        f.write(\"--------------------------\")\n",
    "        f.write(\"--- File Created at:  \" + current_date_time) \n",
    "        f.write(\"--------------------------\\n\\n\")\n",
    "    with open(PATH_TO_CV_RESULTS+\"_SVM_build_classifier_results_of_outer_fold.txt\",'w') as f:\n",
    "        f.write(\"--------------------------\")\n",
    "        f.write(\"--- File Created at:  \" + current_date_time) \n",
    "        f.write(\"--------------------------\\n\\n\")\n",
    "        \n",
    "    outer_k_index = 0\n",
    "    for outer_train_index, outer_holdout_index in outer_cv_skf.split(X=arg_train_data, y=encoded_y):\n",
    "        print(\"[DEBUG] starting outer_fold -- {} of {}\". format(outer_k_index+1, outer_k))\n",
    "        print(\"  [TIMESTAMP]\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        cv_X_train = arg_train_data.iloc[outer_train_index]\n",
    "        cv_Y_train = encoded_y.iloc[outer_train_index]\n",
    "        cv_Y_train_unencoded =  label_encoder.inverse_transform(cv_Y_train.values.ravel())\n",
    "        cv_id_train= arg_train_ids.iloc[outer_train_index]\n",
    "        \n",
    "        cv_X_test = arg_train_data.iloc[outer_holdout_index]\n",
    "        cv_Y_test = encoded_y.iloc[outer_holdout_index]\n",
    "        cv_Y_test_unencoded =  label_encoder.inverse_transform(cv_Y_test.values.ravel())\n",
    "        cv_id_test= arg_train_ids.iloc[outer_holdout_index]\n",
    "        \n",
    "        # apply scaling \n",
    "        print (\" calling scale_this_data() on cv_X_train\")\n",
    "#         check_if_any_genes_zero(cv_X_train, cv_id_train, \"notebook11_tcga_cvtrain_before_scaling\")\n",
    "        cv_X_train, cv_scalar = scale_this_data(cv_X_train, None, SCALE_DATA)\n",
    "#         check_if_any_genes_zero(cv_X_train, cv_id_train, \"notebook11_tcga_cvtrain_after_scaling\")\n",
    "    \n",
    "        print (\" calling scale_this_data() on cv_X_test\")\n",
    "#         check_if_any_genes_zero(cv_X_test, cv_id_test, \"notebook11_tcga_cvtest_before_scaling\")\n",
    "        cv_X_test, cv_scalar = scale_this_data(cv_X_test, cv_scalar, SCALE_DATA)\n",
    "#         check_if_any_genes_zero(cv_X_test, cv_id_test, \"notebook11_tcga_cvtest_after_scaling\")\n",
    "        \n",
    "        # apply dimension reduction\n",
    "        report_suffix = \"trainSet_randomOutOf\" + str(outer_k) + \"Folds\"\n",
    "        \n",
    "# -------------- either use this block here       \n",
    "#         print (\" calling apply_feature_reduction() on cv_X_train - UMAP as main reducer\")\n",
    "#         print (\"   (for pca)  just to generate plots, discard pca-transformed data\")\n",
    "#         _, cv_pca = apply_feature_reduction(cv_X_train, cv_Y_train_unencoded, None,\"pca\", report_suffix)\n",
    "#         print (\"   (for umap) will generate plots, keep umap-transformed data\")\n",
    "#         cv_X_train, cv_umap = apply_feature_reduction(cv_X_train, cv_Y_train_unencoded, None,\"umap\", report_suffix)\n",
    "#         print (\" calling apply_feature_reduction() on cv_X_test\")\n",
    "#         print (\"   (for pca)  just to generate plots, discard pca-transformed data\")\n",
    "#         _, cv_pca = apply_feature_reduction(cv_X_test, cv_Y_test_unencoded, cv_pca,\"pca\", report_suffix)\n",
    "#         print (\"   (for umap) will generate plots, keep umap-transformed data\")\n",
    "#         cv_X_test, cv_umap = apply_feature_reduction(cv_X_test, cv_Y_test_unencoded, cv_umap,\"umap\", report_suffix)\n",
    "        \n",
    "# -------------- Or this block here   \n",
    "        print (\" calling apply_feature_reduction() on cv_X_train - PCA as main reducer\")\n",
    "        print (\"   (for umap) just to generate plots, discard umap-transformed data\")\n",
    "        _, cv_umap = apply_feature_reduction(cv_X_train, cv_Y_train_unencoded, None,\"umap\", report_suffix)\n",
    "        print (\"   (for pca)  will generate plots, keep pca-transformed data\")\n",
    "        cv_X_train, cv_pca = apply_feature_reduction(cv_X_train, cv_Y_train_unencoded, None,\"pca\", report_suffix)\n",
    "        print (\" calling apply_feature_reduction() on cv_X_test\")\n",
    "        print (\"   (for umap) just to generate plots, discard umap-transformed data\")\n",
    "        _, cv_umap = apply_feature_reduction(cv_X_test, cv_Y_test_unencoded, cv_umap,\"umap\", report_suffix)\n",
    "        print (\"   (for pca)  will generate plots, keep pca-transformed data\")\n",
    "        cv_X_test, cv_pca = apply_feature_reduction(cv_X_test, cv_Y_test_unencoded, cv_pca,\"pca\", report_suffix)\n",
    "        \n",
    "        \n",
    "        #Construct the grid searches\n",
    "        gs_svm = GridSearchCV(\n",
    "                              estimator=pipe_svm, \n",
    "                              param_grid=grid_params_svm, \n",
    "                              scoring='f1_weighted',\n",
    "                              cv=inner_cv_skf.split(cv_X_train, cv_Y_train.values.ravel()), \n",
    "                              n_jobs=MAX_JOBS, \n",
    "                              verbose=1,        # this int can be up to 10; higher value means more output\n",
    "                              return_train_score=True, \n",
    "                              refit=True    # when refit=True; then can use gs.best_estimator_\n",
    "#                               refit=False\n",
    "                             )\n",
    "\n",
    "        # Maintaining the models name and pipeline name to loop over and save the results\n",
    "        grids = [gs_svm]\n",
    " \n",
    "        for idx, gs in enumerate(grids):\n",
    "            print(\" [DEBUG] Start of loop for ML algorithm type\")\n",
    "            print(\"   [TIMESTAMP]\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "            print('  Classifier: %s' % grids_dict[idx])\n",
    "            # Fit grid search\n",
    "            gs.fit(cv_X_train, cv_Y_train.values.ravel())\n",
    "            print(\"  Done fitting\")\n",
    "            # Best params\n",
    "            print('  Best params: %s' % gs.best_params_)\n",
    "            # Best validation data f1\n",
    "            print('  Best validation F1: %.3f' % gs.best_score_)\n",
    "\n",
    "            meantrainingscore = gs.cv_results_['mean_train_score'][gs.best_index_]\n",
    "            stdtrainingscore = gs.cv_results_['std_train_score'][gs.best_index_]\n",
    "            meanvalidationscore = gs.cv_results_['mean_test_score'][gs.best_index_]\n",
    "            stdvalidationscore = gs.cv_results_['std_test_score'][gs.best_index_]\n",
    "            \n",
    "            # print(gs.best_estimator_.get_params().keys())\n",
    "            inner_k_index=0\n",
    "            \n",
    "            print(\"    [DEBUG] outer fold starting series of inner folds to figure out hyperparameters\")\n",
    "            for inner_train_index, validation_index in inner_cv_skf.split(cv_X_train, cv_Y_train):\n",
    "                print(\"     [DEBUG] inside outer_fold -- {} of {} --  inner_fold {} of {}\". format(outer_k_index+1, outer_k, inner_k_index+1, inner_k))\n",
    "\n",
    "                gs.estimator.set_params(**gs.best_params_).fit(cv_X_train.iloc[inner_train_index], cv_Y_train.iloc[inner_train_index].values.ravel()) \n",
    "                ypred = gs.estimator.predict(cv_X_train.iloc[validation_index])                      #https://stackoverflow.com/questions/51091132/pandas-and-scikit-learn-keyerror-not-in-index\n",
    "                #print(\"  [DEBUG]ypred: {}\".format(ypred))\n",
    "                training_report_cv = classification_report(label_encoder.inverse_transform(cv_Y_train.iloc[validation_index].values.ravel()), label_encoder.inverse_transform(ypred))\n",
    "                f1_cv_macro = f1_score(cv_Y_train.iloc[validation_index].values.ravel(), ypred, average='macro')\n",
    "                f1_cv_weighted = f1_score(cv_Y_train.iloc[validation_index].values.ravel(), ypred, average='weighted')\n",
    "\n",
    "                with open(PATH_TO_CV_RESULTS+\"_SVM_build_classifier_results_of_inner_fold.txt\",'a+') as f:\n",
    "                    f.write(\" +++++ inner fold k=\" +str(inner_k_index+1) + \" of outer fold k=\" +str(outer_k_index+1) + \" ++++ \\n\")  \n",
    "                    f.write(grids_dict[idx] + \" CLASSIFICATION REPORT during CV on TRAIN DATA #{} \\n\".format(outer_k_index+1))\n",
    "                    f.write(str(training_report_cv) + \"\\n\")\n",
    "#                     f.write(grids_dict[idx] + \" PERFORMANCE METRIC (f1-macro)    during CV on TRAIN DATA #{}: {} \\n\".format(\n",
    "#                                                                                     outer_k_index+1,\n",
    "#                                                                                     str(f1_cv_macro))\n",
    "                    f.write(grids_dict[idx] + \" PERFORMANCE METRIC (f1-weighted) during CV on TRAIN DATA #{}: {} \\n\".format(\n",
    "                                                                                    outer_k_index+1,\n",
    "                                                                                    str(f1_cv_weighted)))\n",
    "                            \n",
    "                       \n",
    "                            \n",
    "                    f.write(\"----------------------------------\\n\\n\")\n",
    "                    \n",
    "                inner_k_index += 1\n",
    "                \n",
    "                print(\"     [DEBUG]   end of inner loop iteration -- \")\n",
    "            print(\"    [DEBUG] done with this series of inner folds\")\n",
    "\n",
    "                        \n",
    "            # Predict on test data with best params\n",
    "            gs.estimator.set_params(**gs.best_params_).fit(cv_X_train, cv_Y_train.values.ravel()) \n",
    "            cv_Y_pred = gs.estimator.predict(cv_X_test)\n",
    "            \n",
    "            # Performance metrics\n",
    "            f1_macro = str(f1_score(cv_Y_test, cv_Y_pred, average='macro'))   # no longer used        \n",
    "            f1_weighted = str(f1_score(cv_Y_test, cv_Y_pred, average='weighted')) \n",
    "            acc_balanced  = str(balanced_accuracy_score( cv_Y_test, cv_Y_pred))\n",
    "            matthews_coef = str(matthews_corrcoef(       cv_Y_test, cv_Y_pred))\n",
    "      \n",
    "            print(\"    Evaluation set for best params -- F1 score (weighted): {} \".format(f1_weighted))\n",
    "            print(\"    Evaluation set for best params -- Balanced ACC: {} \".format(acc_balanced))\n",
    "            print(\"    Evaluation set for best params -- Matthews Coef: {} \".format(matthews_coef))\n",
    "\n",
    "            # Print and Save test result on evaluation data\n",
    "            print(\"    Saving results to file at path:\" + PATH_TO_CV_RESULTS)\n",
    "            test_report_evaluation = classification_report(label_encoder.inverse_transform(cv_Y_test.values.ravel()), label_encoder.inverse_transform(cv_Y_pred))              #https://joshlawman.com/metrics-classification-report-breakdown-precision-recall-f1/\n",
    "            test_con_matrix_evaluation = confusion_matrix(label_encoder.inverse_transform(cv_Y_test.values.ravel()), label_encoder.inverse_transform(cv_Y_pred))                    #https://stats.stackexchange.com/questions/95209/how-can-i-interpret-sklearn-confusion-matrix\n",
    "            with open(PATH_TO_CV_RESULTS+\"_SVM_build_classifier_results_of_outer_fold.txt\",'a+') as f:\n",
    "                f.write(\" +++++ best fold of ++ of outer fold k=\" +str(outer_k_index+1) + \"++++ \\n\")  \n",
    "                f.write(grids_dict[idx]+\" CLASSIFICATION REPORT during CV on TRAIN DATA #{}\\n\".format(\n",
    "                                                                                    outer_k_index+1))\n",
    "                f.write(str(test_report_evaluation) + \"\\n\" )\n",
    "#                 f.write(grids_dict[idx]+\" PERFORMANCE METRIC (f1-macro)    during CV on TRAIN DATA: \"  + str(f1_macro) + \"\\n\")\n",
    "#                 f.write(\"grids_dict[idx]+\" \"PERFORMANCE METRIC (f1-weighted) during CV on TRAIN DATA #{}: {} \\n\\n\".format(\n",
    "#                                                                                     outer_k_index+1,\n",
    "#                                                                                     str(f1_weighted)))\n",
    "    \n",
    "                f.write(\"OVERALL PERFORMANCE METRICS ON TRAIN DATA -- outer fold CV #{} \\n\".format(outer_k_index+1))\n",
    "                f.write(\"  f1-weighted:   \"  + f1_weighted + \"\\n\")\n",
    "                f.write(\"  acc-balanced:  \"  + acc_balanced + \"\\n\")\n",
    "                f.write(\"  matthews_coef: \"  + matthews_coef + \"\\n\")\n",
    "                f.write(\"\\n\")     \n",
    "    \n",
    "    \n",
    "                f.write(grids_dict[idx]+\" CONFUSION MATRIX\" + \"\\n\")\n",
    "                f.write(str(test_con_matrix_evaluation) + '\\n\\n')\n",
    "                f.write(\"----------------------------------\\n\\n\")\n",
    "            nested_scores[idx][outer_k_index] = f1_weighted #should be macro or weighted?\n",
    "            non_nested_scores[idx][outer_k_index] = meanvalidationscore\n",
    "\n",
    "            \n",
    "            # (updated to save every outer_fold )\n",
    "            # - involves changing the 'current_best_*' variables to be list of size `outer_k_index`\n",
    "\n",
    "            print(\"    Saving best model/scalar/umap/pca for outer_fold #{}\". format(outer_k_index+1))\n",
    "#                 best_gs_models[idx] = gs.estimator\n",
    "#                 current_best_model = gs.estimator\n",
    "            current_best_model.append(gs.best_estimator_)  \n",
    "            current_best_scalar.append( cv_scalar)\n",
    "            current_best_umap.append(cv_umap)\n",
    "            current_best_pca.append(cv_pca)\n",
    "            print(\"      current_best_model :\", current_best_model[outer_k_index])     \n",
    "            print(\"        n_features:\", current_best_model[outer_k_index].n_features_in_)        \n",
    "            print(\"      current_best_scalar :\", current_best_scalar[outer_k_index])      \n",
    "            print(\"      current_best_umap :\", current_best_umap[outer_k_index])     \n",
    "            print(\"        n_features:\", cv_X_test.shape[1]) \n",
    "            \n",
    "            \n",
    "#             if (f1_weighted == max(nested_scores[idx,:])): \n",
    "#                 print(\"    Saving this model, scalar, and reducer objects since it has current max score of nested cv\")\n",
    "# #                 best_gs_models[idx] = gs.estimator\n",
    "# #                 current_best_model = gs.estimator\n",
    "#                 current_best_model = gs.best_estimator_  # this only works when refit parameter is true\n",
    "#                 current_best_scalar = cv_scalar\n",
    "#                 current_best_umap = cv_umap\n",
    "#                 current_best_pca = cv_pca\n",
    "#                 print(\"      current_best_model :\", current_best_model)     \n",
    "#                 print(\"        n_features:\", current_best_model.n_features_in_)        \n",
    "#                 print(\"      current_best_scalar :\", current_best_scalar)      \n",
    "#                 print(\"      current_best_umap :\", current_best_umap)     \n",
    "#                 print(\"        n_features:\", cv_X_test.shape[1]) \n",
    "             \n",
    "            print(\" [DEBUG] End of loop for ML algorithm type -- this currently just SVM \")\n",
    "            \n",
    "        outer_k_index += 1\n",
    "        print(\"[DEBUG] End of outer loop iteration -- \\n\\n\\n\\n\")\n",
    "    \n",
    "    return current_best_model, current_best_scalar, current_best_umap, current_best_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b35d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how called\n",
    "\"\"\" \n",
    "    # this is a void function, expect new files created in folder: classification_PREDICTIONS\n",
    "    apply_model( _TODO___)\n",
    "\"\"\"\n",
    "# (some sources) \n",
    "# - #https://joshlawman.com/metrics-classification-report-breakdown-precision-recall-f1/\n",
    "# - #https://stats.stackexchange.com/questions/95209/how-can-i-interpret-sklearn-confusion-matrix\n",
    "\n",
    "#APPLY MODEL TO DATASETS TO ASSESS TESTING CLASSIFICATION ACCURACY\n",
    "def apply_model_to_assess_performance(arg_outer_k_index, arg_best_model, arg_best_scalar, arg_best_umap, arg_best_pca, arg_test_data, arg_test_labels, arg_test_ids, arg_report_suffix):\n",
    "\n",
    "    print(\"  checking info of some arguments \")  \n",
    "    print(\"    arg_outer_k_index : \", arg_outer_k_index)     \n",
    "    print(\"    arg_best_model : \", arg_best_model)     \n",
    "    print(\"      n_features: \", arg_best_model.n_features_in_)        \n",
    "    print(\"    arg_best_scalar : \", arg_best_scalar)      \n",
    "    print(\"    arg_best_umap : \", arg_best_umap)     \n",
    "    print(\"    arg_best_pca : \", arg_best_pca)   \n",
    "    \n",
    "    # create the label_encoder based on the arg_report_stuff\n",
    "    #### _TODO_\n",
    "    # for now assume this\n",
    "#    label_encoder = LabelEncoder().fit(arg_test_labels)\n",
    "    \n",
    "    # create new file to keep track of this result\n",
    "    write_filename = PATH_TO_PREDICTIONS \\\n",
    "                    + \"_SVM_apply_classifier_results_for_\" \\\n",
    "                    + arg_report_suffix \\\n",
    "                    + \"_\" \\\n",
    "                    + str(arg_outer_k_index+1) \\\n",
    "                    + \"_\" \\\n",
    "                    + \".txt\"\n",
    "            \n",
    "    with open(write_filename,'w') as f:\n",
    "        f.write(\"--------------------------\")\n",
    "        f.write(\"--- File Created at:  \" + current_date_time) \n",
    "        f.write(\"--------------------------\\n\\n\")\n",
    "        \n",
    "    # create variables for the excel files too   \n",
    "    write_excel_name_con_matrix = PATH_TO_PREDICTIONS_EXCEL_FILES \\\n",
    "                    + \"con_matrix_for_\" \\\n",
    "                    + arg_report_suffix \\\n",
    "                    + \"_\" \\\n",
    "                    + str(arg_outer_k_index+1) \\\n",
    "                    + \"_\" \\\n",
    "                    + \".xlsx\"\n",
    "    \n",
    "    write_excel_name_class_wise_metrics = PATH_TO_PREDICTIONS_EXCEL_FILES \\\n",
    "                    + \"classWiseMetrics_for_\" \\\n",
    "                    + arg_report_suffix \\\n",
    "                    + \"_\" \\\n",
    "                    + str(arg_outer_k_index+1) \\\n",
    "                    + \"_\" \\\n",
    "                    + \".xlsx\"\n",
    "        \n",
    "\n",
    "    # (consider) try-catch statement that includes reverse of this function to open the .pkl file\n",
    "    ## joblib.dump(best_gs_models[idx], PATH_TO_MODEL_NAME + '_' + grids_dict[idx] + '.pkl', compress=1)\n",
    "\n",
    "    # apply scaling and dimension reduction here with best scalar and pca objects from above\n",
    "        \n",
    "    print (\" calling scale_this_data() from apply_model_to_assess_performance() on\", arg_report_suffix) \n",
    "    check_if_any_genes_zero(arg_test_data, arg_test_ids, \"\".join([\"notebook11_\",arg_report_suffix,\"_\",\"before_scaling\"]))\n",
    "    test_data_after_scalar, best_scalar = scale_this_data(arg_test_data, arg_best_scalar, SCALE_DATA)\n",
    "    check_if_any_genes_zero(test_data_after_scalar, arg_test_ids, \"\".join([\"notebook11_\",arg_report_suffix,\"_\",\"after_scaling\"]))\n",
    "\n",
    "    \n",
    "    report_suffix = arg_report_suffix\n",
    "    \n",
    "    # -------------- either use this block here            \n",
    "#     print (\" calling apply_feature_reduction() from apply_model_to_assess_performance(UMAP) on\", arg_report_suffix)\n",
    "#     print (\"   (for pca)  just to generate plots, discard pca-transformed data\")\n",
    "#     _, best_pca = apply_feature_reduction(test_data_after_scalar, arg_test_labels, arg_best_pca,\"pca\", report_suffix)\n",
    "#     print (\"   (for umap) will generate plots, keep umap-transformed data\")\n",
    "#     test_data_after_reducer, best_umap = apply_feature_reduction(test_data_after_scalar, arg_test_labels, arg_best_umap,\"umap\", report_suffix)\n",
    "\n",
    "    # -------------- Or this block here  \n",
    "    print (\" calling apply_feature_reduction() from apply_model_to_assess_performance(PCA) on\", arg_report_suffix)\n",
    "    print (\"   (for umap) will generate plots, keep umap-transformed data\")\n",
    "    _, best_umap = apply_feature_reduction(test_data_after_scalar, arg_test_labels, arg_best_umap,\"umap\", report_suffix)\n",
    "    print (\"   (for pca)  just to generate plots, discard pca-transformed data\")\n",
    "    test_data_after_reducer, best_pca = apply_feature_reduction(test_data_after_scalar, arg_test_labels, arg_best_pca, \"pca\", report_suffix)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"    Making the prediction with the fitted model\")\n",
    "    print(\"      begin call arg_best_model.predict()\")\n",
    "    test_pred = arg_best_model.predict(test_data_after_reducer)\n",
    " \n",
    "    print(\"    Preparing the predictions just returned and ground truth label values to be in same format\")\n",
    "    predicted_labels = pd.DataFrame(data = label_encoder.inverse_transform(test_pred))\n",
    "    ground_truth = pd.DataFrame(data = arg_test_labels)\n",
    "\n",
    "    print(\"    Generating results using sklearn modules for predictions compared to ground truth labels\")\n",
    "    test_report   = classification_report(       ground_truth, predicted_labels)  \n",
    "    f1_macro      = str(f1_score(                ground_truth, predicted_labels, average='macro'))\n",
    "    f1_weighted   = str(f1_score(                ground_truth, predicted_labels, average='weighted'))\n",
    "    acc_balanced  = str(balanced_accuracy_score( ground_truth, predicted_labels))\n",
    "    matthews_coef = str(matthews_corrcoef(       ground_truth, predicted_labels))\n",
    "    con_matrix  = confusion_matrix(ground_truth, predicted_labels, labels=np.unique(labels))      \n",
    "    con_matrix = np.float64(con_matrix)\n",
    "    \n",
    "#     print(\"    Creating merged_array of predictions and truth values to know size of confusion matrix\")\n",
    "#     merged_array_labels_of_pred_and_truth = []\n",
    "#     merged_array_labels_of_pred_and_truth.extend(np.unique(label_encoder.inverse_transform(test_pred)))\n",
    "#     merged_array_labels_of_pred_and_truth.extend(np.unique(arg_test_labels.to_numpy()))\n",
    "#     print(\"      (# unique labels in test_pred) \", np.unique(label_encoder.inverse_transform(test_pred)))\n",
    "#     print(\"      (# unique labels in arg_test_labels) \", np.unique(arg_test_labels.to_numpy()))\n",
    "#     print(\"      (# unique labels in merged_array) \", np.unique(merged_array_labels_of_pred_and_truth))\n",
    "    \n",
    "    print(\"    Saving single confusion matrix as excel table for primary/derived metrics\")\n",
    "    # convert the sklearn confusion matrix object into a dataframe that can be written to an excel file\n",
    "    # (source) https://stackoverflow.com/questions/60841348/how-to-convert-confusion-matrix-to-dataframe\n",
    "    df_con_matrix = pd.DataFrame(con_matrix, columns = np.unique(labels))    \n",
    "    df_con_matrix.to_excel(write_excel_name_con_matrix)\n",
    "    \n",
    "    print(\"    Exploring the confusion matrix for primary/derived metrics\")\n",
    "    # (source) https://stackoverflow.com/questions/39770376/scikit-learn-get-accuracy-scores-for-each-class\n",
    "    results_metrics = []\n",
    "    results_derived = []\n",
    "    results_derived2 = []\n",
    "    results_both_metrics = []\n",
    "    \n",
    "    for class_id in label_encoder.transform(np.unique(labels)):\n",
    "        \n",
    "        class_label = np.unique(labels)[class_id]   \n",
    "        print (\"      (working on class_label) \", class_label)\n",
    "        \n",
    "        # figure out how many of this class there is\n",
    "        # (source) https://stackoverflow.com/questions/49471442/using-pandas-value-counts-to-get-one-value\n",
    "        test_labels_df =  pd.DataFrame(arg_test_labels, columns = ['label'])  \n",
    "        countDat = test_labels_df['label'].value_counts()\n",
    "        try:\n",
    "            class_count =  countDat[class_label]      \n",
    "        except:\n",
    "            class_count = 0   # in situations where the label isn't predicted or actual, need this try-except block                               \n",
    "        print(\"         print(class_count):\", class_count)\n",
    "\n",
    "        \n",
    "        # figure out primary metrics\n",
    "        TP = con_matrix[class_id,class_id]\n",
    "        FN = np.sum(con_matrix[class_id]) - TP\n",
    "        FP = np.sum(con_matrix[:,class_id]) - TP\n",
    "        TN = np.sum(con_matrix) - TP - FN - FP\n",
    "\n",
    "        # build matrix for primary metrics counts \n",
    "        results_metrics.append([\n",
    "                                        class_label,\n",
    "                                        class_count,\n",
    "                                        TP,\n",
    "                                        FN,\n",
    "                                        FP,\n",
    "                                        TN\n",
    "                               ])\n",
    "        \n",
    "        \n",
    "        # figure out derived metrics\n",
    "        #   sensitivity = 0 if TP == 0\n",
    "        if TP != 0:\n",
    "            sensitivity = TP/(TP+FN)\n",
    "        else:\n",
    "            sensitivity = 0.\n",
    "        specificity = TN/(TN+FP)\n",
    "        FPR = 1 - sensitivity\n",
    "        FNR = 1 - specificity\n",
    "        PPV = TP/(TP+FP)\n",
    "        NPV = TN/(TN+FN)\n",
    "        accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "        prevalence = (TP+FN)/(TN+TP+FP+FN)\n",
    "        f_score = 2*(PPV*sensitivity)/(PPV+sensitivity)\n",
    "        \n",
    "        # build matrix for derived metrics\n",
    "        results_derived.append([\n",
    "                                        class_label,\n",
    "                                        PPV,\n",
    "                                        NPV,\n",
    "                                        accuracy,\n",
    "                                        prevalence\n",
    "                               ])\n",
    "        results_derived2.append([\n",
    "                                        class_label,\n",
    "                                        sensitivity,\n",
    "                                        specificity,\n",
    "                                        FPR,\n",
    "                                        FNR,\n",
    "                                        f_score\n",
    "                               ])\n",
    "        \n",
    "        # build matrix for derived metrics\n",
    "        results_both_metrics.append([\n",
    "                                        class_label,\n",
    "                                        class_count,\n",
    "                                        TP,\n",
    "                                        FN,\n",
    "                                        FP,\n",
    "                                        TN,\n",
    "                                        PPV,\n",
    "                                        NPV,\n",
    "                                        accuracy,\n",
    "                                        prevalence,\n",
    "                                        sensitivity,\n",
    "                                        specificity,\n",
    "                                        FPR,\n",
    "                                        FNR,\n",
    "                                        f_score\n",
    "                               ])\n",
    "        \n",
    "        print (\"        end of iteration on: \", class_label)\n",
    "    \n",
    "    print(\"    Saving single excel table for primary/derived metrics\")\n",
    "    output_both_metrics = pd.DataFrame(results_both_metrics,columns = [\n",
    "                                        'class',\n",
    "                                        'count',\n",
    "                                        'TP',\n",
    "                                        'FN',\n",
    "                                        'FP',\n",
    "                                        'TN',\n",
    "                                        'PPV',\n",
    "                                        'NPV',\n",
    "                                        'accuracy',\n",
    "                                        'prevalence',\n",
    "                                        'sensitivity',\n",
    "                                        'specificity',\n",
    "                                        'FPR',\n",
    "                                        'FNR',\n",
    "                                        'f1-score'\n",
    "                                                ])  \n",
    "    output_both_metrics.to_excel(write_excel_name_class_wise_metrics)\n",
    "    \n",
    "    print(\"    Printing the overall performance metrics for \"+arg_report_suffix+\" #\"+str(arg_outer_k_index+1)+\" :\")\n",
    "#     print('      F1 score (macro):     ' + f1_macro)\n",
    "    print('      F1 score (weighted):  ' + f1_weighted)\n",
    "    print('      ACC-balanced:         ' + acc_balanced)\n",
    "    print('      MCC (matthews_coef):  ' + matthews_coef)\n",
    "    \n",
    "    print(\"    Saving results to file at path:\" + PATH_TO_PREDICTIONS)\n",
    "    \n",
    "    # create the dataframe objects for each matrix made in the loop, add a header too \n",
    "    output_metrics = pd.DataFrame(results_metrics,columns = [\n",
    "                                        'class',\n",
    "                                        'count',\n",
    "                                        'TP',\n",
    "                                        'FN',\n",
    "                                        'FP',\n",
    "                                        'TN'\n",
    "                                                ])    \n",
    "    output_derived = pd.DataFrame(results_derived,columns = [\n",
    "                                        'class',\n",
    "                                        'PPV',\n",
    "                                        'NPV',\n",
    "                                        'accuracy',\n",
    "                                        'prevalence'\n",
    "                                                ])    \n",
    "    output_derived2 = pd.DataFrame(results_derived2,columns = [\n",
    "                                        'class',\n",
    "                                        'sensitivity',\n",
    "                                        'specificity',\n",
    "                                        'FPR',\n",
    "                                        'FNR',\n",
    "                                        'f1-score'\n",
    "                                                ])    \n",
    "    \n",
    "    with open(write_filename,'a+') as f:\n",
    "        f.write(\"SVM CLASSIFICATION REPORT on TEST DATA \" + arg_report_suffix + \" #\" + str(arg_outer_k_index+1) + \"\\n\")\n",
    "        f.write(str(test_report) + \"\\n\" )\n",
    "        f.write(\"OVERALL PERFORMANCE METRICS on TEST DATA \" + arg_report_suffix + \" #\" + str(arg_outer_k_index+1) + \"\\n\")\n",
    "#         f.write(\"  f1-macro:      \"  + f1_macro + \"\\n\")\n",
    "        f.write(\"  f1-weighted:   \"  + f1_weighted + \"\\n\")\n",
    "        f.write(\"  acc-balanced:  \"  + acc_balanced + \"\\n\")\n",
    "        f.write(\"  matthews_coef: \"  + matthews_coef + \"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"CONFUSION MATRIX\" + \"\\n\")\n",
    "        f.write(str(con_matrix) + '\\n')\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"INDIVIDUAL PERFORMANCE METRICS (custom calculations) \\n\")\n",
    "        f.write(str(output_metrics))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(str(output_derived))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(str(output_derived2))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"----------------------------------\\n\\n\")      \n",
    "          \n",
    "    \n",
    "    # (later) can refer to old code to print out performance metrics; in detail \n",
    "    # (later) can refer to old code to print out top 3 best predictions per sample, to investigate\n",
    "          \n",
    "    return \n",
    "    \n",
    "        \n",
    "##### <end of function apply_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa4900b",
   "metadata": {},
   "source": [
    "# Main Function cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "grids_n = 1  # svm\n",
    "grids_dict = {0: 'SVM'}\n",
    "\n",
    "print (\" Calling -- build_classifier_model() to build model/estimator \\n\")\n",
    "print(\"[TIMESTAMP]\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "best_model, best_scalar, best_umap, best_pca = build_classifier_model(X_train, Y_train, id_train) #scaling/DR done at each CV\n",
    "\n",
    "\n",
    "print(\"[TIMESTAMP]\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print (\" done calling -- build_classifier_model()\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113deb2c",
   "metadata": {},
   "source": [
    "### Main Function --- apply fitted model on test set #1: internal evaluation set of 20% TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71b6b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\" Begin main function block --- apply fitted model on test set #1: internal evaluation set of 20% TCGA\")\n",
    "print(\"[TIMESTAMP]\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "outer_k = 5\n",
    "\n",
    "for outer_k_index in range(outer_k):\n",
    "    print (\"  Calling -- apply_model() -- for Test Set #1 on iteration #{} of {}\\n\".format(outer_k_index+1, outer_k))\n",
    "    apply_model_to_assess_performance(\n",
    "                                      outer_k_index,\n",
    "                                      best_model[outer_k_index], \n",
    "                                      best_scalar[outer_k_index], \n",
    "                                      best_umap[outer_k_index], \n",
    "                                      best_pca[outer_k_index], \n",
    "                                      X_test, \n",
    "                                      Y_test, \n",
    "                                      id_test, \n",
    "                                      \"testSet01_internalEvaluation\"\n",
    "                                     ) \n",
    "    print (\"  done calling -- apply_model()\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"[TIMESTAMP]\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print (\" End main function block --- apply fitted model on test set #1: internal evaluation set of 20% TCGA\")\n",
    "\n",
    "\n",
    "\n",
    "# print (\"  Calling -- apply_model() -- for Test Set #1 \\n\")\n",
    "# apply_model_to_assess_performance(best_model, best_scalar, best_umap, best_pca, X_test, Y_test, id_test, \"testSet01_internalEvaluation\") # scaling and DR part of fxn\n",
    "# print (\"  done calling -- apply_model()\")\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c3e8c",
   "metadata": {},
   "source": [
    "### Main Function --- prepare test set #2: external independent set of 100% GTEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa60d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  Before next apply_model() call, need to load independent data and parse into variables\")\n",
    "\n",
    "print(\"  Trying to load gtex data into a Pandas dataframe\")  \n",
    "\n",
    "try:\n",
    "    df = pd.read_table(PATH_TO_GTEX_DATA)\n",
    "    \n",
    "    print (\"    File(s) found\")\n",
    "    print (\"      pd.read_table() into 'df':\", PATH_TO_GTEX_DATA)\n",
    "except:\n",
    "    print (\"    ERROR:  File(s) not found\")\n",
    "    #quit()\n",
    "    \n",
    "print(\"  checking dimensions of the file loaded into df\")  \n",
    "print(\"    df.shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef62be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  tallying the 'label' column of the loaded dataframe\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb2557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  previewing the top of the file\")\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb57d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"  saving non-feature columns (sample_id, label) elsewhere, they need to be dropped from df for other functions\")\n",
    "Y_independent = df['label']\n",
    "id_independent = df['sample_id']\n",
    "\n",
    "df = df.drop('sample_id', axis=1)  # axis=0 refers to vertical/rows\n",
    "df = df.drop('label', axis=1)      # axis=1 refers to horizontal/columns\n",
    "\n",
    "X_independent = df  # notice we pass it here to match up how rest of code works, just different test set\n",
    "\n",
    "print(\"  checking dimensions of the new variables \")  \n",
    "print(\"    X_independent.shape :\", X_independent.shape,  \"   # should be 100% of gtex data\")\n",
    "print(\"    Y_independent.shape :\", Y_independent.shape,  \"   # should be 100% of gtex labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fc4ee",
   "metadata": {},
   "source": [
    "### Main Function --- apply fitted model on test set #2: external independent set of 100% GTEx¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\" Begin main function block --- apply fitted model on test set #2: external independent set of 100% GTEx\")\n",
    "print(\"[TIMESTAMP]\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "outer_k = 5\n",
    "\n",
    "for outer_k_index in range(outer_k):\n",
    "    print (\"  Calling -- apply_model() -- for Test Set #2 on iteration #{} of {}\\n\".format(outer_k_index+1, outer_k))\n",
    "    apply_model_to_assess_performance(\n",
    "                                      outer_k_index,\n",
    "                                      best_model[outer_k_index], \n",
    "                                      best_scalar[outer_k_index], \n",
    "                                      best_umap[outer_k_index], \n",
    "                                      best_pca[outer_k_index], \n",
    "                                      X_independent, \n",
    "                                      Y_independent, \n",
    "                                      id_independent, \n",
    "                                      \"testSet02_externalIndependent\"\n",
    "                                     ) # scaling and DR part of fxn\n",
    "    print (\"  done calling -- apply_model()\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"[TIMESTAMP]\", datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print (\" End main function block --- apply fitted model on test set #2: external independent set of 100% GTEx\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"  Calling -- apply_model() -- for Test Set #2 \\n\")\n",
    "# apply_model_to_assess_performance(best_model, best_scalar, best_umap, best_pca, X_independent, Y_independent, id_independent, \"testSet02_externalIndependent\")\n",
    "# print (\"  done calling -- apply_model() \")\n",
    "# print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
